{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from time import time\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "Download the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.5,), (0.5,)),\n",
    "                              ])\n",
    "\n",
    "# Prep the datasets\n",
    "trainset = datasets.MNIST('MnistData', download=True, train=True, transform=transform)\n",
    "valset = datasets.MNIST('MnistData', download=True, train=False, transform=transform)\n",
    "\n",
    "# Feeding for a NN\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
    "valloader = torch.utils.data.DataLoader(valset, batch_size=64, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "Basic CNN model and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class mnistnet(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden1, hidden2, output_size):\n",
    "        super(mnistnet, self).__init__()\n",
    "        # Define the layers\n",
    "        self.conv1 = nn.Conv2d(1, 3, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(3, 9, 5)\n",
    "        self.fl1 = nn.Linear(input_size, hidden1)\n",
    "        self.fl2 = nn.Linear(hidden1, hidden2)\n",
    "        self.fl3 = nn.Linear(hidden2, output_size)\n",
    "        self.out = nn.LogSoftmax(dim=1)\n",
    "        \n",
    "    # Forward pass\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 9 * 4 * 4)\n",
    "        x = F.relu(self.fl1(x))\n",
    "        x = F.relu(self.fl2(x))\n",
    "        x = self.fl3(x)\n",
    "        return self.out(x)\n",
    "    \n",
    "my_network = mnistnet(9 * 4 * 4, 64, 32, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 loss 23.008240826427937"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(my_network.parameters(), lr=0.003, momentum=0.9)\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "losses = []\n",
    "for epoch in range(10):\n",
    "    running_loss = 0\n",
    "    for images, labels in trainloader:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = my_network.forward(images)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        \n",
    "    losses.append(running_loss)\n",
    "    print(\"\\rEpoch {} loss {}\".format(epoch+1,running_loss), end=\"\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Score Cam implementation\n",
    "\n",
    "Score cam is implemented in steps, the method can be found in \n",
    "\n",
    "https://arxiv.org/pdf/1910.01279.pdf\n",
    "\n",
    "1. Start by propagating a input image $x$ past the last conv-layer and extract $k$ number of feature maps\n",
    "\n",
    "2. Upscale those feature maps to the original image size\n",
    "\n",
    "3. Normalize each mask with $A^k = \\frac{A^k_{i,j}}{max(A^k) - min(A^k)}$\n",
    "\n",
    "4. Project maps to original image and pass thru the network as $f(x \\otimes A)$\n",
    "\n",
    "5. Obtain a weight $w^k$ for each of the masks defined as NN output \n",
    "\n",
    "6. Calculate final mask as $c_{final}=ReLU(\\sum w^k * A^k)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x125490940>"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "conv_part = [my_network._modules[l] for l in [\"conv1\", \"pool\", \"conv2\"]]\n",
    "image_index = 10\n",
    "\n",
    "test_img = images[image_index].reshape(1,1,28,28)\n",
    "test_target = labels[image_index].item()\n",
    "\n",
    "# 1. Pass input image only thru the convolutional layers\n",
    "x = conv_part[1](F.relu(conv_part[0](test_img)))\n",
    "x = conv_part[1](F.relu(conv_part[2](x)))\n",
    "x = F.interpolate(x, size=(28,28), mode='bilinear')\n",
    "_,k,_,_ = x.shape\n",
    "\n",
    "# 2. Normalize the masks\n",
    "# 3. Project masks to original image\n",
    "masked_images = []\n",
    "for i in range(k):\n",
    "    maxA = max(x[0,i,:].flatten())\n",
    "    minA = min(x[0,i,:].flatten())\n",
    "    x[0,i,:] /= (maxA-minA + 1e-4)\n",
    "    masked_images.append(test_img * x[:,i,:,:])\n",
    "    \n",
    "# 4. Pass masks through the original network and calculate score\n",
    "weight_scores = [i[test_target] for i in np.exp(my_network(torch.cat(masked_images)).detach().numpy())]\n",
    "\n",
    "# 5. Construct the final mask to be projected\n",
    "final_mask = torch.Tensor(np.zeros((28,28)))\n",
    "for i in range(k):\n",
    "    final_mask += F.relu(x[0,i,:] * torch.Tensor([weight_scores[i]]))\n",
    "\n",
    "# Visualize result\n",
    "plt.figure()\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(test_img.reshape(28,28), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(final_mask.detach().numpy().reshape(28,28), cmap=\"gray\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "final_cam = test_img.detach().numpy().reshape(28,28) * final_mask.detach().numpy()\n",
    "plt.imshow(final_cam, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the b/w image is hard to see where the filter works\n",
    "\n",
    "we can instead use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x124f24f60>"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "inp = test_img.detach().numpy().reshape(28,28)\n",
    "empty_img = np.zeros((28,28,3))\n",
    "\n",
    "for i in range(28):\n",
    "    for j in range(28):\n",
    "        if inp[i][j] == -1:\n",
    "            empty_img[i][j][0] = 1\n",
    "        else:\n",
    "            empty_img[i][j][1] = 1\n",
    "            \n",
    "for i in range(3):\n",
    "    empty_img[:,:,i] *= final_mask.detach().numpy()\n",
    "plt.imshow(empty_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mask = (x[0,:,:,:].view(28,28,9) * torch.Tensor(weight_scores)).sum(dim=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
